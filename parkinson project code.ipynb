{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb793246-74aa-487a-bb08-e166ba0d094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data DATA [--outdir OUTDIR]\n",
      "                             [--test-size TEST_SIZE] [--seed SEED] [--cv CV]\n",
      "                             [--plot]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# `train.py`\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def load_dataset(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    # Standard UCI file often uses 'name' as an ID column; drop if present\n",
    "    if 'name' in df.columns:\n",
    "        df = df.drop(columns=['name'])\n",
    "    # Expect 'status' as target: 1 = PD, 0 = healthy\n",
    "    if 'status' not in df.columns:\n",
    "        raise ValueError(\"Expected target column 'status' not found.\")\n",
    "    y = df['status'].astype(int)\n",
    "    X = df.drop(columns=['status'])\n",
    "    # Keep only numeric columns\n",
    "    num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    X = X[num_cols]\n",
    "    return X, y, num_cols\n",
    "\n",
    "def build_pipelines(numeric_features):\n",
    "    numeric_proc = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_proc, numeric_features)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    svm_clf = Pipeline(steps=[\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "\n",
    "    rf_clf = Pipeline(steps=[\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"svm_rbf\": svm_clf,\n",
    "        \"random_forest\": rf_clf\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, name, outdir, plot=False):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Probabilities for ROC-AUC\n",
    "    if hasattr(model.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # e.g., SVC with probability=True supports predict_proba; otherwise fallback to decision_function\n",
    "        if hasattr(model.named_steps[\"clf\"], \"decision_function\"):\n",
    "            scores = model.decision_function(X_test)\n",
    "            # Min-max to [0,1] for ROC convenience\n",
    "            s_min, s_max = scores.min(), scores.max()\n",
    "            y_proba = (scores - s_min) / (s_max - s_min + 1e-12)\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"precision\": float(precision_score(y_test, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_test, y_pred, zero_division=0)),\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        metrics[\"roc_auc\"] = float(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    # Plots\n",
    "    if plot:\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        fig_cm, ax_cm = plt.subplots()\n",
    "        im = ax_cm.imshow(cm, interpolation=\"nearest\")\n",
    "        ax_cm.set_title(f\"Confusion Matrix - {name}\")\n",
    "        ax_cm.set_xlabel(\"Predicted\")\n",
    "        ax_cm.set_ylabel(\"True\")\n",
    "        for (i, j), v in np.ndenumerate(cm):\n",
    "            ax_cm.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "        fig_cm.tight_layout()\n",
    "        fig_cm.savefig(os.path.join(outdir, f\"confusion_matrix_{name}.png\"), dpi=150)\n",
    "        plt.close(fig_cm)\n",
    "\n",
    "        # ROC curve\n",
    "        if y_proba is not None:\n",
    "            fig_roc, ax_roc = plt.subplots()\n",
    "            RocCurveDisplay.from_predictions(y_test, y_proba, name=name, ax=ax_roc)\n",
    "            ax_roc.set_title(f\"ROC Curve - {name}\")\n",
    "            fig_roc.tight_layout()\n",
    "            fig_roc.savefig(os.path.join(outdir, f\"roc_{name}.png\"), dpi=150)\n",
    "            plt.close(fig_roc)\n",
    "\n",
    "    return metrics, y_pred, y_proba, model\n",
    "\n",
    "def cross_validate(model, X, y, cv):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring=\"f1\")\n",
    "    return float(scores.mean()), float(scores.std())\n",
    "\n",
    "def save_feature_importance_if_any(model, X_columns, outdir, tag):\n",
    "    # Try to extract feature importances (only for tree models)\n",
    "    try:\n",
    "        clf = model.named_steps[\"clf\"]\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            # Need to map back to original numeric columns after preprocessing\n",
    "            importances = clf.feature_importances_\n",
    "            fi = pd.DataFrame({\n",
    "                \"feature\": X_columns,\n",
    "                \"importance\": importances\n",
    "            }).sort_values(\"importance\", ascending=False)\n",
    "            fi.to_csv(os.path.join(outdir, f\"feature_importance_{tag}.csv\"), index=False)\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train PD detection models on voice features.\")\n",
    "    parser.add_argument(\"--data\", type=str, required=True, help=\"Path to parkinsons.csv\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"./artifacts\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test size ratio\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    parser.add_argument(\"--cv\", type=int, default=5, help=\"K-folds for cross-validation\")\n",
    "    parser.add_argument(\"--plot\", action=\"store_true\", help=\"Save confusion matrix and ROC plots\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    Path(args.outdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    X, y, num_cols = load_dataset(args.data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
    "    )\n",
    "\n",
    "    models = build_pipelines(num_cols)\n",
    "\n",
    "    all_results = {}\n",
    "    best_name = None\n",
    "    best_f1 = -1.0\n",
    "    best_model = None\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Cross-val on train for robust selection\n",
    "        cv_mean, cv_std = cross_validate(model, X_train, y_train, cv=args.cv)\n",
    "\n",
    "        # Fit and evaluate on holdout test\n",
    "        metrics, y_pred, y_proba, fitted = evaluate_model(\n",
    "            model, X_train, X_test, y_train, y_test, name, args.outdir, plot=args.plot\n",
    "        )\n",
    "\n",
    "        result = {\n",
    "            \"cv_f1_mean\": cv_mean,\n",
    "            \"cv_f1_std\": cv_std,\n",
    "            **metrics\n",
    "        }\n",
    "        all_results[name] = result\n",
    "\n",
    "        if metrics[\"f1\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1\"]\n",
    "            best_name = name\n",
    "            best_model = fitted\n",
    "\n",
    "    # Save metrics\n",
    "    with open(os.path.join(args.outdir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "    # Save a text report\n",
    "    lines = [\"Model comparison (higher is better):\\n\"]\n",
    "    for name, res in all_results.items():\n",
    "        lines.append(\n",
    "            f\"- {name}: \"\n",
    "            f\"F1={res.get('f1'):.4f}, \"\n",
    "            f\"Acc={res.get('accuracy'):.4f}, \"\n",
    "            f\"Prec={res.get('precision'):.4f}, \"\n",
    "            f\"Rec={res.get('recall'):.4f}, \"\n",
    "            f\"ROC-AUC={res.get('roc_auc', float('nan')):.4f}, \"\n",
    "            f\"CV(F1)={res.get('cv_f1_mean'):.4f}Â±{res.get('cv_f1_std'):.4f}\"\n",
    "        )\n",
    "    lines.append(f\"\\nBest model: {best_name} (F1={best_f1:.4f})\")\n",
    "    report_path = os.path.join(args.outdir, \"report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    # Save best model\n",
    "    model_path = os.path.join(args.outdir, \"best_model.joblib\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    # Try to save feature importance if RF won\n",
    "    saved_fi = save_feature_importance_if_any(best_model, num_cols, args.outdir, best_name)\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(f\"\\nSaved best model to: {model_path}\")\n",
    "    print(f\"Saved metrics to: {os.path.join(args.outdir, 'metrics.json')}\")\n",
    "    print(f\"Saved report to: {report_path}\")\n",
    "    if args.plot:\n",
    "        print(\"Saved confusion matrix and ROC plots.\")\n",
    "    if saved_fi:\n",
    "        print(f\"Saved feature_importance_{best_name}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1200337-fd36-48b4-8a4d-25137f695759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
